{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMAGD - 20 news dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UcOPoLTScu4V"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.admagd import ADMAGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_body = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author(text):\n",
    "    match = re.search(r\"From: (.+?)(\\n|$)\", text)\n",
    "    if match:\n",
    "        author = match.group(1)\n",
    "        return re.sub(r\"[<>].*\", \"\", author).strip()\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [extract_author(doc) for doc in newsgroups.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "author2doc = {}\n",
    "\n",
    "for doc_id, author in enumerate(authors):\n",
    "    if author not in author2doc:\n",
    "        author2doc[author] = []\n",
    "    author2doc[author].append(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP WORDS creation\n",
    "Initial stop words from `sklearn.feature_extraction._stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS # Total 318 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop_words = list(set(stopwords.words('english')))\n",
    "sk_stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "STOP_WORDS = list(set(nltk_stop_words + sk_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS += [\"use\", \"think\", \"thanks\", \"know\", \"like\", \"make\", \"say\", \"time\", \"use\", \"need\", \"want\", \"come\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer\n",
    "Lemmatizer minimizes text ambiguity. Example words like bicycle or bicycles are converted to base word bicycle. Basically, it will convert all words having the same meaning but different representation to their base form. It reduces the word density in the given text and helps in preparing the accurate features for training machine. Cleaner the data, the more intelligent and accurate your machine learning model, will be. NLTK Lemmatizer will also saves memory as well as computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger') # need for pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Creating a POS tag map\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "def convertWordIntoLemmatizeWord(words):\n",
    "  return [lemmatizer.lemmatize(word, tag_map[tag[0]]) for word, tag in pos_tag(words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a pre-process function\n",
    "* Remove numbers. ✅\n",
    "* Convert word into lowercase word. ✅\n",
    "* Remove all stop words. ✅\n",
    "* Remove all punctuations. ✅\n",
    "* Some white spaces may be added to the list of words, due to the translate function & nature of our documents. Remove them as well. ✅\n",
    "* Remove just-numeric strings. ✅\n",
    "* Lemmatize. ✅\n",
    "* Remove words with only 2 characters or less. [Low frequency] ✅\n",
    "* Remove words with more than 12 characters. [High frequency] ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "  #First, remove numbers\n",
    "  words = [re.sub(r\"\\d+\", \"\", word) for word in words]\n",
    "  #Normalize the cases of our words\n",
    "  words = [word.lower() for word in words]\n",
    "  #Remove all punctuations\n",
    "  table = str.maketrans('', '', punctuation)\n",
    "  words = [word.translate(table) for word in words]\n",
    "  #Some white spaces may be added to the list of words, due to the translate function & nature of our documents. We've to remove them.\n",
    "  words = [word for word in words if word]\n",
    "  #Remove just-numeric strings\n",
    "  words = [word for word in words if not word.isdigit()]\n",
    "  #Remove all stop words\n",
    "  words = [word for word in words if word not in STOP_WORDS]\n",
    "  #Lemmatize\n",
    "  words = convertWordIntoLemmatizeWord(words)\n",
    "  #Remove all stop words\n",
    "  words = [word for word in words if word not in STOP_WORDS]\n",
    "  #Remove words with less than 3 characters and more than 20 characters\n",
    "  words = [word for word in words if len(word) > 2 and len(word) <= 20]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(docs):\n",
    "  preprocessed_docs = []\n",
    "  for doc in docs:\n",
    "    words = word_tokenize(doc)\n",
    "    words = preprocess(words)\n",
    "    # preprocessed_docs.append(\" \".join(words))\n",
    "    preprocessed_docs.append(words)\n",
    "  return preprocessed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = preprocess_documents(newsgroup_body.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized the data\n",
    "# vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features=1000)\n",
    "# X = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# # Convert matrix to list of tuples (document, word count)\n",
    "# corpus = [list(zip(row.indices, row.data)) for row in X]\n",
    "\n",
    "# # id2word mapping\n",
    "# id2word = {v: k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "# tfidf_X = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# # Create the gensim dictionary manually\n",
    "# id2word_gensim = {i: token for token, i in tfidf_vectorizer.vocabulary_.items()}\n",
    "# gensim_dictionary = corpora.Dictionary.from_corpus(Sparse2Corpus(tfidf_X, documents_columns=False), id2word=id2word_gensim)\n",
    "\n",
    "# # Convert the TF-IDF matrix to a gensim corpus.\n",
    "# corpus_gensim = Sparse2Corpus(tfidf_X, documents_columns=False)  # documents_columns=False for compatibility with gensim\n",
    "\n",
    "# # Ensure the dictionary is updated with the current corpus\n",
    "# gensim_dictionary.id2token = id2word_gensim\n",
    "# gensim_dictionary.token2id = tfidf_vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize alpha, beta, a, and b if different from the defaults\n",
    "alpha_init = 0.1\n",
    "beta_init = 0.1\n",
    "a_init = 0.1\n",
    "b_init = 0.1\n",
    "\n",
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ADMAGD model\n",
    "model = ADMAGD(corpus=corpus_tfidf, num_topics=num_topics, id2word=dictionary, authors=author2doc, alpha_init=alpha_init, beta_init=beta_init, a_init=a_init, b_init=b_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "iteration: 20\n",
      "iteration: 21\n",
      "iteration: 22\n",
      "iteration: 23\n",
      "iteration: 24\n",
      "iteration: 25\n",
      "iteration: 26\n",
      "iteration: 27\n",
      "iteration: 28\n",
      "iteration: 29\n",
      "iteration: 30\n",
      "iteration: 31\n",
      "iteration: 32\n",
      "iteration: 33\n",
      "iteration: 34\n",
      "iteration: 35\n",
      "iteration: 36\n",
      "iteration: 37\n",
      "iteration: 38\n",
      "iteration: 39\n",
      "iteration: 40\n",
      "iteration: 41\n",
      "iteration: 42\n",
      "iteration: 43\n",
      "iteration: 44\n",
      "iteration: 45\n",
      "iteration: 46\n",
      "iteration: 47\n",
      "iteration: 48\n",
      "iteration: 49\n",
      "iteration: 50\n",
      "iteration: 51\n",
      "iteration: 52\n",
      "iteration: 53\n",
      "iteration: 54\n",
      "iteration: 55\n",
      "iteration: 56\n",
      "iteration: 57\n",
      "iteration: 58\n",
      "iteration: 59\n",
      "iteration: 60\n",
      "iteration: 61\n",
      "iteration: 62\n",
      "iteration: 63\n",
      "iteration: 64\n",
      "iteration: 65\n",
      "iteration: 66\n",
      "iteration: 67\n",
      "iteration: 68\n",
      "iteration: 69\n",
      "iteration: 70\n",
      "iteration: 71\n",
      "iteration: 72\n",
      "iteration: 73\n",
      "iteration: 74\n",
      "iteration: 75\n",
      "iteration: 76\n",
      "iteration: 77\n",
      "iteration: 78\n",
      "iteration: 79\n",
      "iteration: 80\n",
      "iteration: 81\n",
      "iteration: 82\n",
      "iteration: 83\n",
      "iteration: 84\n",
      "iteration: 85\n",
      "iteration: 86\n",
      "iteration: 87\n",
      "iteration: 88\n",
      "iteration: 89\n",
      "iteration: 90\n",
      "iteration: 91\n",
      "iteration: 92\n",
      "iteration: 93\n",
      "iteration: 94\n",
      "iteration: 95\n",
      "iteration: 96\n",
      "iteration: 97\n",
      "iteration: 98\n",
      "iteration: 99\n",
      "iteration: 100\n",
      "iteration: 101\n",
      "iteration: 102\n",
      "iteration: 103\n",
      "iteration: 104\n",
      "iteration: 105\n",
      "iteration: 106\n",
      "iteration: 107\n",
      "iteration: 108\n",
      "iteration: 109\n",
      "iteration: 110\n",
      "iteration: 111\n",
      "iteration: 112\n",
      "iteration: 113\n",
      "iteration: 114\n",
      "iteration: 115\n",
      "iteration: 116\n",
      "iteration: 117\n",
      "iteration: 118\n",
      "iteration: 119\n",
      "iteration: 120\n",
      "iteration: 121\n",
      "iteration: 122\n",
      "iteration: 123\n",
      "iteration: 124\n",
      "iteration: 125\n",
      "iteration: 126\n",
      "iteration: 127\n",
      "iteration: 128\n",
      "iteration: 129\n",
      "iteration: 130\n",
      "iteration: 131\n",
      "iteration: 132\n",
      "iteration: 133\n",
      "iteration: 134\n",
      "iteration: 135\n",
      "iteration: 136\n",
      "iteration: 137\n",
      "iteration: 138\n",
      "iteration: 139\n",
      "iteration: 140\n",
      "iteration: 141\n",
      "iteration: 142\n",
      "iteration: 143\n",
      "iteration: 144\n",
      "iteration: 145\n",
      "iteration: 146\n",
      "iteration: 147\n",
      "iteration: 148\n",
      "iteration: 149\n",
      "iteration: 150\n",
      "iteration: 151\n",
      "iteration: 152\n",
      "iteration: 153\n",
      "iteration: 154\n",
      "iteration: 155\n",
      "iteration: 156\n",
      "iteration: 157\n",
      "iteration: 158\n",
      "iteration: 159\n",
      "iteration: 160\n",
      "iteration: 161\n",
      "iteration: 162\n",
      "iteration: 163\n",
      "iteration: 164\n",
      "iteration: 165\n",
      "iteration: 166\n",
      "iteration: 167\n",
      "iteration: 168\n",
      "iteration: 169\n",
      "iteration: 170\n",
      "iteration: 171\n",
      "iteration: 172\n",
      "iteration: 173\n",
      "iteration: 174\n",
      "iteration: 175\n",
      "iteration: 176\n",
      "iteration: 177\n",
      "iteration: 178\n",
      "iteration: 179\n",
      "iteration: 180\n",
      "iteration: 181\n",
      "iteration: 182\n",
      "iteration: 183\n",
      "iteration: 184\n",
      "iteration: 185\n",
      "iteration: 186\n",
      "iteration: 187\n",
      "iteration: 188\n",
      "iteration: 189\n",
      "iteration: 190\n",
      "iteration: 191\n",
      "iteration: 192\n",
      "iteration: 193\n",
      "iteration: 194\n",
      "iteration: 195\n",
      "iteration: 196\n",
      "iteration: 197\n",
      "iteration: 198\n",
      "iteration: 199\n",
      "iteration: 200\n"
     ]
    }
   ],
   "source": [
    "# Run Gibbs sampling\n",
    "model.gibbs_sampling(iterations=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"tfidf_train_extra_stopwords_200_iteration_admagd_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"trained_ model/{model_file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_ model/tfidf_train_extra_stopwords_200_iteration_admagd_model.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, f\"trained_ model/{model_file_name}.joblib\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UcOPoLTScu4V",
    "S_BBNjjzc4m5",
    "c0cAeBowGUVP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lda-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1669881b8e0ee381f1d44208a6e6b4675430ed382f288976bd9acdbb8db18405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
