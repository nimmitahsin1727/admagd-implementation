{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMAGD - 20 news dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UcOPoLTScu4V"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.admagd import ADMAGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', categories = categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author(text):\n",
    "    match = re.search(r\"From: (.+?)(\\n|$)\", text)\n",
    "    if match:\n",
    "        author = match.group(1)\n",
    "        return re.sub(r\"[<>].*\", \"\", author).strip()\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [extract_author(doc) for doc in newsgroups.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "author2doc = {}\n",
    "\n",
    "for doc_id, author in enumerate(authors):\n",
    "    if author not in author2doc:\n",
    "        author2doc[author] = []\n",
    "    author2doc[author].append(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP WORDS creation\n",
    "Initial stop words from `sklearn.feature_extraction._stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS # Total 318 words\n",
    "\n",
    "stopWords=list(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some more common stop words throughout all docs which doesn't play no part in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords+=['subject','from', 'date', 'reply-to', 'newsgroups', 'message-id', 'lines', 'path', 'organization', \n",
    "            'would', 'writes', 'references', 'article', 'sender', 'nntp-posting-host', 'people', \n",
    "            'university', 'think', 'xref', 'cantaloupe.srv.cs.cmu.edu', 'could', 'distribution', 'first', \n",
    "            'anyone', 'really', 'since', 'believe', 'still', \n",
    "            \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\"]\n",
    "\n",
    "stopWords = set(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer\n",
    "Lemmatizer minimizes text ambiguity. Example words like bicycle or bicycles are converted to base word bicycle. Basically, it will convert all words having the same meaning but different representation to their base form. It reduces the word density in the given text and helps in preparing the accurate features for training machine. Cleaner the data, the more intelligent and accurate your machine learning model, will be. NLTK Lemmatizer will also saves memory as well as computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger') # need for pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Creating a POS tag map\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "def convertWordIntoLemmatizeWord(words):\n",
    "  return [lemmatizer.lemmatize(word, tag_map[tag[0]]) for word, tag in pos_tag(words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a pre-process function\n",
    "* Remove numbers. ✅\n",
    "* Convert word into lowercase word. ✅\n",
    "* Remove all stop words. ✅\n",
    "* Remove all punctuations. ✅\n",
    "* Some white spaces may be added to the list of words, due to the translate function & nature of our documents. Remove them as well. ✅\n",
    "* Remove just-numeric strings. ✅\n",
    "* Lemmatize. ✅\n",
    "* Remove words with only 2 characters or less. [Low frequency] ✅\n",
    "* Remove words with more than 12 characters. [High frequency] ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "  #Remove all stop words\n",
    "  words = [word for word in words if word not in stopWords]\n",
    "  #First, remove numbers\n",
    "  words = [re.sub(r\"\\d+\", \"\", word) for word in words]\n",
    "  #Normalize the cases of our words\n",
    "  words = [word.lower() for word in words]\n",
    "  #Remove all punctuations\n",
    "  table = str.maketrans('', '', punctuation)\n",
    "  words = [word.translate(table) for word in words]\n",
    "  #Some white spaces may be added to the list of words, due to the translate function & nature of our documents. We've to remove them.\n",
    "  words = [word for word in words if word]\n",
    "  #Remove just-numeric strings\n",
    "  words = [word for word in words if not word.isdigit()]\n",
    "  #Lemmatize\n",
    "  words = convertWordIntoLemmatizeWord(words)\n",
    "  #Remove words with less than 3 characters and more than 12 characters\n",
    "  words = [word for word in words if len(word) > 2 and len(word) <= 12]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(docs):\n",
    "  preprocessed_docs = []\n",
    "  for doc in docs:\n",
    "    words = word_tokenize(doc)\n",
    "    words = preprocess(words)\n",
    "    preprocessed_docs.append(\" \".join(words))\n",
    "  return preprocessed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = preprocess_documents(newsgroups.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized the data\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# Convert matrix to list of tuples (document, word count)\n",
    "corpus = [list(zip(row.indices, row.data)) for row in X]\n",
    "\n",
    "# id2word mapping\n",
    "id2word = {v: k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize alpha, beta, a, and b if different from the defaults\n",
    "alpha_init = 0.1\n",
    "beta_init = 0.1\n",
    "a_init = 0.1\n",
    "b_init = 0.1\n",
    "\n",
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ADMAGD model\n",
    "model = ADMAGD(corpus=corpus, num_topics=num_topics, id2word=id2word, authors=author2doc, alpha_init=alpha_init, beta_init=beta_init, a_init=a_init, b_init=b_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "iteration: 20\n",
      "iteration: 21\n",
      "iteration: 22\n",
      "iteration: 23\n",
      "iteration: 24\n",
      "iteration: 25\n",
      "iteration: 26\n",
      "iteration: 27\n",
      "iteration: 28\n",
      "iteration: 29\n",
      "iteration: 30\n",
      "iteration: 31\n",
      "iteration: 32\n",
      "iteration: 33\n",
      "iteration: 34\n",
      "iteration: 35\n",
      "iteration: 36\n",
      "iteration: 37\n",
      "iteration: 38\n",
      "iteration: 39\n",
      "iteration: 40\n",
      "iteration: 41\n",
      "iteration: 42\n",
      "iteration: 43\n",
      "iteration: 44\n",
      "iteration: 45\n",
      "iteration: 46\n",
      "iteration: 47\n",
      "iteration: 48\n",
      "iteration: 49\n",
      "iteration: 50\n",
      "iteration: 51\n",
      "iteration: 52\n",
      "iteration: 53\n",
      "iteration: 54\n",
      "iteration: 55\n",
      "iteration: 56\n",
      "iteration: 57\n",
      "iteration: 58\n",
      "iteration: 59\n",
      "iteration: 60\n",
      "iteration: 61\n",
      "iteration: 62\n",
      "iteration: 63\n",
      "iteration: 64\n",
      "iteration: 65\n",
      "iteration: 66\n",
      "iteration: 67\n",
      "iteration: 68\n",
      "iteration: 69\n",
      "iteration: 70\n",
      "iteration: 71\n",
      "iteration: 72\n",
      "iteration: 73\n",
      "iteration: 74\n",
      "iteration: 75\n",
      "iteration: 76\n",
      "iteration: 77\n",
      "iteration: 78\n",
      "iteration: 79\n",
      "iteration: 80\n",
      "iteration: 81\n",
      "iteration: 82\n",
      "iteration: 83\n",
      "iteration: 84\n",
      "iteration: 85\n",
      "iteration: 86\n",
      "iteration: 87\n",
      "iteration: 88\n",
      "iteration: 89\n",
      "iteration: 90\n",
      "iteration: 91\n",
      "iteration: 92\n",
      "iteration: 93\n",
      "iteration: 94\n",
      "iteration: 95\n",
      "iteration: 96\n",
      "iteration: 97\n",
      "iteration: 98\n",
      "iteration: 99\n"
     ]
    }
   ],
   "source": [
    "# Run Gibbs sampling\n",
    "model.gibbs_sampling(iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_ model/admagd_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_ model/admagd_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_ model/admagd_model.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dump(model, 'trained_ model/admagd_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load('trained_ model/admagd_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you've run Gibbs sampling\n",
    "word_topic_matrix = loaded_model.word_topic_matrix\n",
    "word_topic_sum = word_topic_matrix.sum(axis=1)[:, np.newaxis]\n",
    "word_topic_dist = word_topic_matrix / word_topic_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: like, just, use, know, apr, distribution, university, say, dod, good, make, work, thing, need, time, new, usa, want, look, year \n",
      "\n",
      "Topic 2: university, israel, say, know, just, apr, like, state, israeli, right, year, use, jew, arab, make, time, want, world, way, jewish \n",
      "\n",
      "Topic 3: use, university, know, like, just, need, work, thanks, problem, want, computer, good, distribution, time, help, run, try, replyto, drive, apr \n",
      "\n",
      "Topic 4: university, use, like, know, just, work, time, need, new, distribution, thanks, say, good, look, want, make, try, usa, problem, question \n",
      "\n",
      "Topic 5: say, just, government, use, like, state, make, know, gun, right, time, university, way, distribution, apr, good, day, thing, law, want \n",
      "\n",
      "Topic 6: university, use, know, distribution, thanks, email, like, computer, look, just, apr, new, work, usa, want, science, problem, time, help, need \n",
      "\n",
      "Topic 7: university, know, just, like, use, good, say, time, apr, distribution, want, new, look, car, make, year, computer, need, way, right \n",
      "\n",
      "Topic 8: use, university, program, like, know, version, work, email, thanks, need, new, help, file, software, distribution, look, available, run, window, just \n",
      "\n",
      "Topic 9: say, university, know, make, just, like, case, apr, thing, use, time, mean, way, world, point, want, god, opinion, distribution, right \n",
      "\n",
      "Topic 10: university, know, like, distribution, just, use, look, email, make, new, good, need, want, thanks, time, replyto, year, work, say, usa \n",
      "\n",
      "Topic 11: game, university, team, play, year, like, win, good, player, time, apr, say, just, know, come, make, fan, season, hockey, look \n",
      "\n",
      "Topic 12: university, use, know, apr, just, distribution, like, message, say, gmt, make, work, xnewsreader, inreplyto, look, try, write, time, good, way \n",
      "\n",
      "Topic 13: university, say, just, like, know, god, time, come, apr, thing, good, make, right, want, way, use, look, tell, year, point \n",
      "\n",
      "Topic 14: know, like, university, use, say, just, good, time, science, make, problem, replyto, apr, way, try, computer, question, thing, look, univ \n",
      "\n",
      "Topic 15: world, long, distribution, way, work, today, government, close, time, state, year, turkish, single, pass, serve, replyto, road, say, exist, armenian \n",
      "\n",
      "Topic 16: university, use, distribution, know, thanks, email, like, just, work, new, drive, usa, need, computer, problem, world, want, apr, sale, make \n",
      "\n",
      "Topic 17: university, like, say, just, know, apr, good, use, new, time, distribution, thing, look, make, state, year, come, want, work, way \n",
      "\n",
      "Topic 18: use, university, know, window, problem, thanks, like, work, just, run, need, file, email, program, look, try, computer, make, help, version \n",
      "\n",
      "Topic 19: use, chip, clipper, key, just, encryption, know, like, work, distribution, government, university, make, say, good, netcomcom, new, need, information, time \n",
      "\n",
      "Topic 20: say, god, know, just, like, christian, time, come, make, way, good, thing, mean, point, jesus, use, question, want, university, word \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize the top N words for each topic\n",
    "N_TOP_WORDS = 20\n",
    "for i in range(loaded_model.num_topics):\n",
    "    top_words_idx = word_topic_dist[i].argsort()[-N_TOP_WORDS:][::-1]\n",
    "    top_words = [loaded_model.id2word[idx] for idx in top_words_idx]\n",
    "    print(f\"Topic {i + 1}: {', '.join(top_words)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the author-topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the author_topic_matrix to get author-topic distribution\n",
    "\n",
    "# Compute the sum of rows in author_topic_matrix\n",
    "author_topic_sum = loaded_model.author_topic_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Replace zero sums with a small epsilon value\n",
    "epsilon = 1e-10\n",
    "author_topic_sum[author_topic_sum == 0] = epsilon\n",
    "\n",
    "# Perform element-wise division\n",
    "author_topic_dist = loaded_model.author_topic_matrix / author_topic_sum\n",
    "\n",
    "# Visualize the top N topics for each author\n",
    "N_TOP_TOPICS = 2\n",
    "top_topics_list = []\n",
    "for i, author in enumerate(loaded_model.authors):\n",
    "    top_topics_idx = author_topic_dist[i].argsort()[-N_TOP_TOPICS:][::-1]\n",
    "    top_topics_list.append(top_topics_idx)\n",
    "    # print(f\"Author {i+1} => {author} : Topic IDs {top_topics_idx} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mamatha Devineni Ratnam</td>\n",
       "      <td>[10, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)</td>\n",
       "      <td>[5, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hilmi-er@dsv.su.se (Hilmi Eren)</td>\n",
       "      <td>[1, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guyd@austin.ibm.com (Guy Dawson)</td>\n",
       "      <td>[2, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander Samuel McDiarmid</td>\n",
       "      <td>[2, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>pcarmack@gimp.kpc.com (Phil Carmack)</td>\n",
       "      <td>[5, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8530</th>\n",
       "      <td>gt5735a@prism.gatech.EDU (Mark Devaney)</td>\n",
       "      <td>[0, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8531</th>\n",
       "      <td>pkeenan@s.psych.uiuc.edu (Patricia Keenan)</td>\n",
       "      <td>[10, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8532</th>\n",
       "      <td>CCMB</td>\n",
       "      <td>[0, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>shaig@composer.think.com (Shai Guday)</td>\n",
       "      <td>[1, 19]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8534 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                authors    topics\n",
       "0                               Mamatha Devineni Ratnam  [10, 19]\n",
       "1     mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)   [5, 19]\n",
       "2                       hilmi-er@dsv.su.se (Hilmi Eren)   [1, 14]\n",
       "3                      guyd@austin.ibm.com (Guy Dawson)   [2, 15]\n",
       "4                            Alexander Samuel McDiarmid    [2, 5]\n",
       "...                                                 ...       ...\n",
       "8529               pcarmack@gimp.kpc.com (Phil Carmack)   [5, 19]\n",
       "8530            gt5735a@prism.gatech.EDU (Mark Devaney)   [0, 18]\n",
       "8531         pkeenan@s.psych.uiuc.edu (Patricia Keenan)  [10, 19]\n",
       "8532                                               CCMB   [0, 18]\n",
       "8533              shaig@composer.think.com (Shai Guday)   [1, 19]\n",
       "\n",
       "[8534 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics_of_authors_df = pd.DataFrame({'authors': loaded_model.authors, 'topics': top_topics_list})\n",
    "top_topics_of_authors_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UcOPoLTScu4V",
    "S_BBNjjzc4m5",
    "c0cAeBowGUVP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lda-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1669881b8e0ee381f1d44208a6e6b4675430ed382f288976bd9acdbb8db18405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
