{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMAGD - Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UcOPoLTScu4V"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"nips_1935_iteration_100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"nips/trained_ model/{model_file_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_path}.pkl\", 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model_joblib = load(f\"{model_path}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_topic_word_distribution(model):\n",
    "#     phi = model.calculate_phi_update()  # This gives you the word-topic matrix\n",
    "\n",
    "#     for topic_idx in range(model.num_topics):\n",
    "#         print(f\"Topic #{topic_idx+1}:\\n\")\n",
    "        \n",
    "#         for word_id in range(model.vocab_size):\n",
    "#             word_probability = phi[topic_idx, word_id]\n",
    "#             word = model.id2word[word_id]\n",
    "#             print(f\"{word}: {word_probability:.4f}\")\n",
    "        \n",
    "#         print(\"\\n\\n\")  # Print a newline to separate topics\n",
    "\n",
    "# print_topic_word_distribution(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< Topic # 1 >>\n",
      "dynamic: 0.0031\n",
      "temporal: 0.0029\n",
      "nature: 0.0027\n",
      "neuroscience: 0.0027\n",
      "region: 0.0027\n",
      "brain: 0.0026\n",
      "fig: 0.0026\n",
      "signal: 0.0026\n",
      "trajectory: 0.0026\n",
      "activity: 0.0025\n",
      "\n",
      "\n",
      "<< Topic # 2 >>\n",
      "noise: 0.0033\n",
      "recover: 0.0031\n",
      "dimensional: 0.0030\n",
      "iid: 0.0025\n",
      "entry: 0.0025\n",
      "highdimensional: 0.0025\n",
      "row: 0.0025\n",
      "noisy: 0.0025\n",
      "furthermore: 0.0024\n",
      "signal: 0.0024\n",
      "\n",
      "\n",
      "<< Topic # 3 >>\n",
      "classification: 0.0029\n",
      "produce: 0.0029\n",
      "discuss: 0.0028\n",
      "gain: 0.0027\n",
      "evaluation: 0.0027\n",
      "bias: 0.0027\n",
      "easily: 0.0026\n",
      "final: 0.0026\n",
      "bad: 0.0025\n",
      "issue: 0.0025\n",
      "\n",
      "\n",
      "<< Topic # 4 >>\n",
      "efficiently: 0.0036\n",
      "easy: 0.0035\n",
      "analyze: 0.0032\n",
      "lemma: 0.0032\n",
      "acm: 0.0031\n",
      "transaction: 0.0031\n",
      "program: 0.0031\n",
      "outperform: 0.0031\n",
      "nsf: 0.0030\n",
      "cluster: 0.0030\n",
      "\n",
      "\n",
      "<< Topic # 5 >>\n",
      "recognition: 0.0048\n",
      "vision: 0.0047\n",
      "image: 0.0042\n",
      "cvpr: 0.0042\n",
      "object: 0.0038\n",
      "visual: 0.0037\n",
      "convolutional: 0.0032\n",
      "pixel: 0.0031\n",
      "extract: 0.0029\n",
      "classification: 0.0028\n",
      "\n",
      "\n",
      "<< Topic # 6 >>\n",
      "norm: 0.0047\n",
      "convex: 0.0045\n",
      "descent: 0.0040\n",
      "minimization: 0.0035\n",
      "regularization: 0.0034\n",
      "operator: 0.0032\n",
      "regularize: 0.0031\n",
      "continuous: 0.0030\n",
      "converges: 0.0030\n",
      "write: 0.0029\n",
      "\n",
      "\n",
      "<< Topic # 7 >>\n",
      "intelligence: 0.0032\n",
      "determine: 0.0030\n",
      "node: 0.0028\n",
      "graph: 0.0027\n",
      "tree: 0.0025\n",
      "search: 0.0025\n",
      "share: 0.0025\n",
      "artificial: 0.0024\n",
      "associate: 0.0024\n",
      "probabilistic: 0.0024\n",
      "\n",
      "\n",
      "<< Topic # 8 >>\n",
      "bengio: 0.0052\n",
      "deep: 0.0049\n",
      "architecture: 0.0047\n",
      "layer: 0.0045\n",
      "preprint: 0.0042\n",
      "hinton: 0.0040\n",
      "hidden: 0.0034\n",
      "unit: 0.0033\n",
      "kingma: 0.0033\n",
      "recurrent: 0.0033\n",
      "\n",
      "\n",
      "<< Topic # 9 >>\n",
      "lemma: 0.0034\n",
      "upper: 0.0033\n",
      "say: 0.0030\n",
      "expectation: 0.0030\n",
      "arbitrary: 0.0029\n",
      "satisfies: 0.0028\n",
      "question: 0.0028\n",
      "appendix: 0.0027\n",
      "suppose: 0.0027\n",
      "online: 0.0027\n",
      "\n",
      "\n",
      "<< Topic # 10 >>\n",
      "likelihood: 0.0057\n",
      "inference: 0.0051\n",
      "bayesian: 0.0051\n",
      "posterior: 0.0047\n",
      "marginal: 0.0040\n",
      "latent: 0.0039\n",
      "markov: 0.0039\n",
      "density: 0.0038\n",
      "variational: 0.0036\n",
      "family: 0.0035\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words_per_topic(model, top_n=10):\n",
    "    phi = model.calculate_phi_update()  # This gives you the word-topic matrix\n",
    "\n",
    "    for topic_idx in range(model.num_topics):\n",
    "        print(f\"<< Topic # {topic_idx+1} >>\")\n",
    "\n",
    "        # Get the top N word indices for the topic sorted by probability\n",
    "        top_word_indices = phi[topic_idx].argsort()[-top_n:][::-1]\n",
    "        \n",
    "        for word_id in top_word_indices:\n",
    "            word_probability = phi[topic_idx, word_id]\n",
    "            word = model.id2word[word_id]\n",
    "            print(f\"{word}: {word_probability:.4f}\")\n",
    "\n",
    "        print(\"\\n\")  # Print a newline to separate topics\n",
    "\n",
    "# After running your model...\n",
    "print_top_words_per_topic(loaded_model, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you've run Gibbs sampling\n",
    "word_topic_matrix = loaded_model.word_topic_matrix\n",
    "word_topic_sum = word_topic_matrix.sum(axis=1)[:, np.newaxis]\n",
    "word_topic_dist = word_topic_matrix / word_topic_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top N words for each topic\n",
    "N_TOP_WORDS = 50\n",
    "\n",
    "ALL_TOPIC_WORDS = []\n",
    "for i in range(loaded_model.num_topics):\n",
    "    top_words_idx = word_topic_dist[i].argsort()[-N_TOP_WORDS:][::-1]\n",
    "    top_words = [loaded_model.id2word[idx] for idx in top_words_idx]\n",
    "\n",
    "    ALL_TOPIC_WORDS.append(top_words)\n",
    "\n",
    "    # print(f\"Topic {i + 1}: {', '.join(top_words)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic = {}\n",
    "\n",
    "for topic_words in ALL_TOPIC_WORDS:\n",
    "  for word in topic_words:\n",
    "    if word in word_dic:\n",
    "      word_dic[word] = word_dic[word] + 1\n",
    "    else:\n",
    "      word_dic[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the author-topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the author_topic_matrix to get author-topic distribution\n",
    "\n",
    "# Compute the sum of rows in author_topic_matrix\n",
    "author_topic_sum = loaded_model.author_topic_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Replace zero sums with a small epsilon value\n",
    "epsilon = 1e-10\n",
    "author_topic_sum[author_topic_sum == 0] = epsilon\n",
    "\n",
    "# Perform element-wise division\n",
    "author_topic_dist = loaded_model.author_topic_matrix / author_topic_sum\n",
    "\n",
    "# Visualize the top N topics for each author\n",
    "N_TOP_TOPICS = 3\n",
    "top_topics_list = []\n",
    "for i, author in enumerate(loaded_model.authors):\n",
    "    top_topics_idx = author_topic_dist[i].argsort()[-N_TOP_TOPICS:][::-1]\n",
    "    top_topics_list.append(top_topics_idx)\n",
    "    # print(f\"Author {i+1} => {author} : Topic IDs {top_topics_idx} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sebastian Stober</td>\n",
       "      <td>[0, 7, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daniel J. Cameron</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jessica A. Grahn</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aurel A. Lazar</td>\n",
       "      <td>[0, 1, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yevgeniy Slutskiy</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chen-Yu Wei</td>\n",
       "      <td>[8, 2, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yi-Te Hong</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chi-Jen Lu</td>\n",
       "      <td>[9, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Katherine A. Heller</td>\n",
       "      <td>[9, 5, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>David B. Dunson</td>\n",
       "      <td>[1, 9, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               authors     topics\n",
       "0     Sebastian Stober  [0, 7, 6]\n",
       "1    Daniel J. Cameron  [9, 8, 7]\n",
       "2     Jessica A. Grahn  [9, 8, 7]\n",
       "3       Aurel A. Lazar  [0, 1, 6]\n",
       "4    Yevgeniy Slutskiy  [9, 8, 7]\n",
       "5          Chen-Yu Wei  [8, 2, 6]\n",
       "6           Yi-Te Hong  [9, 8, 7]\n",
       "7           Chi-Jen Lu  [9, 8, 7]\n",
       "8  Katherine A. Heller  [9, 5, 7]\n",
       "9      David B. Dunson  [1, 9, 5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics_of_authors_df = pd.DataFrame({'authors': loaded_model.authors, 'topics': top_topics_list})\n",
    "top_topics_of_authors_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>Hongyuan Zha</td>\n",
       "      <td>[9, 6, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          authors     topics\n",
       "655  Hongyuan Zha  [9, 6, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics_of_authors_df[top_topics_of_authors_df['authors'] == 'Hongyuan Zha']\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UcOPoLTScu4V",
    "S_BBNjjzc4m5",
    "c0cAeBowGUVP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lda-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1669881b8e0ee381f1d44208a6e6b4675430ed382f288976bd9acdbb8db18405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
